FedSAC提出了一种新颖的联邦学习（Federated Learning, FL）框架，旨在通过动态子模型分配实现协作公平性（collaborative fairness）。该框架解决了现有方法在分配梯度奖励时忽略局部模型一致性和高贡献客户端多样化需求的问题，这些问题导致了公平性和模型准确性的下降。FedSAC引入了“有界协作公平性”（bounded collaborative fairness, BCF）的概念，并通过理论收敛性保证来支持其有效性。

**核心方法论**

FedSAC主要包含两个核心模块：子模型分配模块和动态聚合模块。

1.  **子模型分配模块（Submodel Allocation Module）**
    该模块的目标是根据客户端的贡献为其分配不同性能的子模型，以实现BCF并保持局部模型之间的一致性。它包含两个步骤：
    *   **神经元重要性评估（Neuron Importance Evaluation）**：
        为了确保子模型包含足够关键的神经元并能产生差异化的性能，FedSAC首先评估模型中每个神经元的重要性。受Taylor-FO方法的启发，神经元的重要性$I_{n_i}$通过移除该神经元后模型损失的变化量来衡量：
        $$I_{n_i} = L_{ce}(V, \theta | \theta^{zero}_{n_i} = 0) - L_{ce}(V, \theta)$$
        其中，$L_{ce}(V, \theta)$是模型$\theta$在验证集$V$上的交叉熵损失，$ \theta^{zero}_{n_i} = 0$表示第$i$个神经元的所有参数被设为0。神经元重要性越大，说明其对模型性能的贡献越大。为了方便子模型的构建，这些重要性分数会被归一化为百分比：
        $$I_{n_i} = \frac{I_{n_i}}{\sum_{n_j \in S} I_{n_j}} \times 100$$
        其中$S$代表模型中的所有神经元。此评估每10个epochs进行一次，以平衡计算开销。
    *   **子模型构建（Submodel Construction）**：
        客户端的贡献度越高，其获得的子模型性能应越好。FedSAC使用客户端的“声誉”（reputation）来动态分配子模型。客户端$i$的声誉$r_i$基于其贡献$c_i$计算，并进行归一化：
        $$r_i = e^{c_i \times \beta}$$
        $$r_i = \frac{r_i}{\max(r)} \times 100$$
        其中$\beta$是一个超参数。为了使高贡献客户端获得更高性能的子模型，FedSAC采用了一种裁剪（pruning）机制。子模型$\theta_i$的构建是通过选择一组神经元实现的，其中包含的神经元数量和重要性程度与客户端的声誉$r_i$成正比：
        $$\theta_i = quantity(r_i, \{I_{n_j}\}_{n_j \in S})$$
        其中$\{I_{n_j}\}_{n_j \in S}$是所有神经元重要性按升序排列的集合。这意味着声誉高的客户端将获得包含更多重要神经元的子模型，从而提高其局部模型的性能，同时确保低贡献客户端的子模型也能包含一定数量的参数，以利于全局模型的训练。

2.  **动态聚合模块（Dynamic Aggregation Module）**
    该模块旨在优化所有神经元的利用率，尤其关注低频神经元的公平对待。传统的FedAvg直接平均上传的局部模型，可能导致不同大小的子模型聚合不公平。FedSAC引入了一种基于神经元被聚合频率的加权机制：
    首先，定义一个掩码（mask）函数$mask^t_i = mask(\theta^t_i, \theta^{t-1}_g)$，它记录了子模型$\theta^t_i$和全局模型$\theta^{t-1}_g$中相同位置的参数。当两个模型在同一位置有参数时，掩码对应位置为1，否则为0。这个掩码实际上计算了每个模型参数在当前轮次被全局模型选中的频率。然后，服务器使用此掩码来动态聚合接收到的子模型：
    $$\theta^{t+1}_g = \frac{\sum_{i \in N} \theta^t_i}{\sum_{i \in N} mask^t_i}$$
    其中$N$是客户端总数。此聚合方式的原理是，某个参数在当前轮次被选中的频率越高，则其在下一轮聚合中的权重越小，从而抑制了高频参数的影响，给予低频神经元更公平的聚合机会，这有助于提高全局模型的整体性能。

**公平性与收敛性理论保证**

FedSAC提供了严格的理论保证。
**公平性保证 (Theorem 1)**：
在$L$-光滑和$\mu$-强凸损失函数的前提下，如果客户端$i$的声誉高于客户端$j$（$r_i \geq r_j$），并且客户端$i$获得的子模型$\theta^t_i$包含客户端$j$的子模型$\theta^t_j$（$\theta^t_j \in \theta^t_i \in \theta^t_g$），那么$\theta^t_i$将比$\theta^t_j$更接近聚合模型$\theta^t_g$，即$||\theta^t_g - \theta^t_i|| \leq ||\theta^t_g - \theta^t_j||$。因此，客户端$i$的损失函数值将小于或等于客户端$j$的损失函数值，即$F(\theta^t_i) \leq F(\theta^t_j)$。这从理论上证明了FedSAC能够奖励高贡献客户端以高性能模型。

**收敛性分析 (Theorem 2)**：
在满足特定假设（包括随机梯度方差受限、随机梯度范数期望受限、以及聚合模型中的每个神经元在长期内被公平分配）的前提下，FedSAC确保聚合模型能够收敛到全局最优解。具体地，其收敛速度为$E[F(\bar{\theta}^T)] - F^* \leq [\frac{L}{\gamma+T}(\frac{2B}{\mu^2} + \frac{\gamma+1}{2} \Delta_1)]$，当$T \to \infty$时，该误差趋近于0。

**复杂度与通信成本**

*   **时间复杂度**：FedSAC的主要计算开销来自于神经元重要性评估，其时间复杂度为$O(M)$，其中$M$是全局模型隐藏层中神经元的总数。
*   **通信成本**：神经元重要性评估在服务器端完成，无需额外的客户端-服务器通信。因此，FedSAC的通信复杂度为每轮$O(d \times m)$，其中$d$是全局模型参数量，$m \le 1$是子模型参数与全局模型参数的平均比率。相比于传统方法的$O(d)$，FedSAC的通信成本更低。

**实验结果**

FedSAC在Fashion MNIST、CIFAR10和SVHN三个公开基准数据集上进行了广泛实验，涵盖了数据量不平衡（POW）、类别数不平衡（CLA）以及数据量和类别数均不平衡（DIR）等异构场景。
*   **公平性**：FedSAC在所有数据集上的公平性（由BCF定义的$\gamma$指标衡量）均高于95.73%，显著优于所有基线方法。尤其在DIR场景下，FedSAC的公平性优势更为明显，例如在CIFAR10的DIR(1.0)场景中，FedSAC的公平性分别比CFFL、CGSV和FedAVE高出69.24%、35.85%和40.71%。
*   **预测性能**：FedSAC在POW和CLA场景下，在所有三个数据集上都取得了最高的测试准确率。在极端非IID设置（如SVHN的DIR(1.0)）下，FedSAC的准确率也显著优于基线方法。这些结果表明FedSAC不仅保证了协作公平性，还提升了整体模型准确率。
*   **消融研究**：实验证明，子模型分配模块对于提升公平性至关重要，而动态聚合模块则显著改善了模型准确率，两者共同构成了FedSAC卓越性能的关键。
*   **可扩展性**：在客户端数量增加（20、40、60）的场景下，FedSAC依然保持了在公平性和准确性方面的领先优势，显示出良好的可扩展性。
*   **超参数$\beta$的影响**：实验表明，较小的$\beta$值（意味着低贡献客户端的子模型包含更多神经元）通常能带来更高的模型准确率。

**结论**

FedSAC通过创新的动态子模型分配策略，有效解决了联邦学习中协作公平性和模型准确性之间的权衡问题。它不仅引入并实现了有界协作公平性，更通过理论分析证明了其公平性和收敛性，并在多项实验中展现出超越现有方法的卓越性能。未来的工作将探索FedSAC在大规模模型上的应用。